{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:49.674154Z","iopub.execute_input":"2025-04-04T09:16:49.674519Z","iopub.status.idle":"2025-04-04T09:16:51.033202Z","shell.execute_reply.started":"2025-04-04T09:16:49.674487Z","shell.execute_reply":"2025-04-04T09:16:51.031713Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Loading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:51.529103Z","iopub.execute_input":"2025-04-04T09:16:51.529669Z","iopub.status.idle":"2025-04-04T09:16:51.577472Z","shell.execute_reply.started":"2025-04-04T09:16:51.529636Z","shell.execute_reply":"2025-04-04T09:16:51.576396Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:52.700029Z","iopub.execute_input":"2025-04-04T09:16:52.700444Z","iopub.status.idle":"2025-04-04T09:16:52.731168Z","shell.execute_reply.started":"2025-04-04T09:16:52.700411Z","shell.execute_reply":"2025-04-04T09:16:52.729691Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:54.294940Z","iopub.execute_input":"2025-04-04T09:16:54.295296Z","iopub.status.idle":"2025-04-04T09:16:54.304016Z","shell.execute_reply.started":"2025-04-04T09:16:54.295266Z","shell.execute_reply":"2025-04-04T09:16:54.302640Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Pregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"pd.set_option('display.float_format', '{:.2f}'.format)\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:56.123583Z","iopub.execute_input":"2025-04-04T09:16:56.123911Z","iopub.status.idle":"2025-04-04T09:16:56.161878Z","shell.execute_reply.started":"2025-04-04T09:16:56.123884Z","shell.execute_reply":"2025-04-04T09:16:56.160778Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin    BMI  \\\ncount       768.00   768.00         768.00         768.00   768.00 768.00   \nmean          3.85   120.89          69.11          20.54    79.80  31.99   \nstd           3.37    31.97          19.36          15.95   115.24   7.88   \nmin           0.00     0.00           0.00           0.00     0.00   0.00   \n25%           1.00    99.00          62.00           0.00     0.00  27.30   \n50%           3.00   117.00          72.00          23.00    30.50  32.00   \n75%           6.00   140.25          80.00          32.00   127.25  36.60   \nmax          17.00   199.00         122.00          99.00   846.00  67.10   \n\n       DiabetesPedigreeFunction    Age  Outcome  \ncount                    768.00 768.00   768.00  \nmean                       0.47  33.24     0.35  \nstd                        0.33  11.76     0.48  \nmin                        0.08  21.00     0.00  \n25%                        0.24  24.00     0.00  \n50%                        0.37  29.00     0.00  \n75%                        0.63  41.00     1.00  \nmax                        2.42  81.00     1.00  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n      <td>768.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.85</td>\n      <td>120.89</td>\n      <td>69.11</td>\n      <td>20.54</td>\n      <td>79.80</td>\n      <td>31.99</td>\n      <td>0.47</td>\n      <td>33.24</td>\n      <td>0.35</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.37</td>\n      <td>31.97</td>\n      <td>19.36</td>\n      <td>15.95</td>\n      <td>115.24</td>\n      <td>7.88</td>\n      <td>0.33</td>\n      <td>11.76</td>\n      <td>0.48</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.08</td>\n      <td>21.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.00</td>\n      <td>99.00</td>\n      <td>62.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>27.30</td>\n      <td>0.24</td>\n      <td>24.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.00</td>\n      <td>117.00</td>\n      <td>72.00</td>\n      <td>23.00</td>\n      <td>30.50</td>\n      <td>32.00</td>\n      <td>0.37</td>\n      <td>29.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.00</td>\n      <td>140.25</td>\n      <td>80.00</td>\n      <td>32.00</td>\n      <td>127.25</td>\n      <td>36.60</td>\n      <td>0.63</td>\n      <td>41.00</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>17.00</td>\n      <td>199.00</td>\n      <td>122.00</td>\n      <td>99.00</td>\n      <td>846.00</td>\n      <td>67.10</td>\n      <td>2.42</td>\n      <td>81.00</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n#     print('==============================')\n#     print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-04T09:16:57.898250Z","iopub.execute_input":"2025-04-04T09:16:57.898674Z","iopub.status.idle":"2025-04-04T09:16:57.908396Z","shell.execute_reply.started":"2025-04-04T09:16:57.898640Z","shell.execute_reply":"2025-04-04T09:16:57.906743Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Data Pre-Processing","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:16:59.913050Z","iopub.execute_input":"2025-04-04T09:16:59.913412Z","iopub.status.idle":"2025-04-04T09:16:59.920179Z","shell.execute_reply.started":"2025-04-04T09:16:59.913384Z","shell.execute_reply":"2025-04-04T09:16:59.919144Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n      dtype='object')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# How many missing zeros are mising in each feature\nfeature_columns = [\n    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'\n]\n\nfor column in feature_columns:\n    print(\"============================================\")\n    print(f\"{column} ==> Missing zeros : {len(df.loc[df[column] == 0])}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:01.129240Z","iopub.execute_input":"2025-04-04T09:17:01.129664Z","iopub.status.idle":"2025-04-04T09:17:01.147274Z","shell.execute_reply.started":"2025-04-04T09:17:01.129631Z","shell.execute_reply":"2025-04-04T09:17:01.145906Z"},"trusted":true},"outputs":[{"name":"stdout","text":"============================================\nPregnancies ==> Missing zeros : 111\n============================================\nGlucose ==> Missing zeros : 5\n============================================\nBloodPressure ==> Missing zeros : 35\n============================================\nSkinThickness ==> Missing zeros : 227\n============================================\nInsulin ==> Missing zeros : 374\n============================================\nBMI ==> Missing zeros : 11\n============================================\nDiabetesPedigreeFunction ==> Missing zeros : 0\n============================================\nAge ==> Missing zeros : 0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n\nfill_values = SimpleImputer(missing_values=0, strategy=\"mean\", copy=False)\ndf[feature_columns] = fill_values.fit_transform(df[feature_columns])\n\nfor column in feature_columns:\n    print(\"============================================\")\n    print(f\"{column} ==> Missing zeros : {len(df.loc[df[column] == 0])}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:03.833168Z","iopub.execute_input":"2025-04-04T09:17:03.833554Z","iopub.status.idle":"2025-04-04T09:17:04.353509Z","shell.execute_reply.started":"2025-04-04T09:17:03.833524Z","shell.execute_reply":"2025-04-04T09:17:04.352519Z"},"trusted":true},"outputs":[{"name":"stdout","text":"============================================\nPregnancies ==> Missing zeros : 0\n============================================\nGlucose ==> Missing zeros : 0\n============================================\nBloodPressure ==> Missing zeros : 0\n============================================\nSkinThickness ==> Missing zeros : 0\n============================================\nInsulin ==> Missing zeros : 0\n============================================\nBMI ==> Missing zeros : 0\n============================================\nDiabetesPedigreeFunction ==> Missing zeros : 0\n============================================\nAge ==> Missing zeros : 0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX = df[feature_columns]\ny = df.Outcome\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:05.192929Z","iopub.execute_input":"2025-04-04T09:17:05.193363Z","iopub.status.idle":"2025-04-04T09:17:05.203781Z","shell.execute_reply.started":"2025-04-04T09:17:05.193330Z","shell.execute_reply":"2025-04-04T09:17:05.202422Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\n\ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_test_pred = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    print(\"TRAINIG RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n\n    print(\"TESTING RESULTS: \\n===============================\")\n    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:07.161132Z","iopub.execute_input":"2025-04-04T09:17:07.161561Z","iopub.status.idle":"2025-04-04T09:17:07.168504Z","shell.execute_reply.started":"2025-04-04T09:17:07.161525Z","shell.execute_reply":"2025-04-04T09:17:07.167252Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Bagging Algorithms\nBootstrap Aggregation or bagging involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n\nThe final output prediction is averaged across the predictions of all of the sub-models.\n\nThe three bagging models covered in this section are as follows:\n\n1. Bagged Decision Trees\n2. Random Forest\n3. Extra Trees","metadata":{}},{"cell_type":"markdown","source":"## 1. Bagged Decision Trees\nBagging performs best with algorithms that have high variance. A popular example is decision trees, often constructed without pruning.\n\n**BaggingClassifier**:\n\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregates their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.\n\n**BaggingClassifier Parameters:**\n- `base_estimator`: The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n***\n- `n_estimators`: The number of base estimators in the ensemble.\n***\n- `max_samples`: The number of samples to draw from X to train each base estimator.\n***\n- `max_features`: The number of features to draw from X to train each base estimator.\n***\n- `bootstrap`: Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n***\n- `bootstrap_features`: Whether features are drawn with replacement.\n***\n- `oob_score`: Whether to use out-of-bag samples to estimate the generalization error.\n***\n- `warm_start`: When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\nbagging_clf = BaggingClassifier(base_estimator=tree, n_estimators=1500, random_state=42)\nbagging_clf.fit(X_train, y_train)\n\nevaluate(bagging_clf, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:11.992838Z","iopub.execute_input":"2025-04-04T09:17:11.993282Z","iopub.status.idle":"2025-04-04T09:17:17.659208Z","shell.execute_reply.started":"2025-04-04T09:17:11.993250Z","shell.execute_reply":"2025-04-04T09:17:17.657884Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"TRAINIG RESULTS: \n===============================\nCONFUSION MATRIX:\n[[349   0]\n [  0 188]]\nACCURACY SCORE:\n1.0000\nCLASSIFICATION REPORT:\n               0      1  accuracy  macro avg  weighted avg\nprecision   1.00   1.00      1.00       1.00          1.00\nrecall      1.00   1.00      1.00       1.00          1.00\nf1-score    1.00   1.00      1.00       1.00          1.00\nsupport   349.00 188.00      1.00     537.00        537.00\nTESTING RESULTS: \n===============================\nCONFUSION MATRIX:\n[[119  32]\n [ 24  56]]\nACCURACY SCORE:\n0.7576\nCLASSIFICATION REPORT:\n               0     1  accuracy  macro avg  weighted avg\nprecision   0.83  0.64      0.76       0.73          0.76\nrecall      0.79  0.70      0.76       0.74          0.76\nf1-score    0.81  0.67      0.76       0.74          0.76\nsupport   151.00 80.00      0.76     231.00        231.00\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"scores = {\n    'Bagging Classifier': {\n        'Train': accuracy_score(y_train, bagging_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, bagging_clf.predict(X_test)),\n    },\n}","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:17.660543Z","iopub.execute_input":"2025-04-04T09:17:17.661012Z","iopub.status.idle":"2025-04-04T09:17:18.144089Z","shell.execute_reply.started":"2025-04-04T09:17:17.660807Z","shell.execute_reply":"2025-04-04T09:17:18.142712Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## 2. Random Forest\n\nA random forest is a meta-estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if `bootstrap=True` (default).\n\n- **Random forest algorithm parameters:**\n- `n_estimators`: The number of trees in the forest.\n*** \n- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n***\n- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then an unlimited number of leaf nodes.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise, it is a leaf.\n***\n- `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n***\n- `oob_score`: Whether to use out-of-bag samples to estimate the generalization accuracy.\n***\n- `warm_start`: When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=42, n_estimators=1000)\nrf_clf.fit(X_train, y_train)\nevaluate(rf_clf, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:18.145627Z","iopub.execute_input":"2025-04-04T09:17:18.146108Z","iopub.status.idle":"2025-04-04T09:17:20.438871Z","shell.execute_reply.started":"2025-04-04T09:17:18.146073Z","shell.execute_reply":"2025-04-04T09:17:20.437701Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TRAINIG RESULTS: \n===============================\nCONFUSION MATRIX:\n[[349   0]\n [  0 188]]\nACCURACY SCORE:\n1.0000\nCLASSIFICATION REPORT:\n               0      1  accuracy  macro avg  weighted avg\nprecision   1.00   1.00      1.00       1.00          1.00\nrecall      1.00   1.00      1.00       1.00          1.00\nf1-score    1.00   1.00      1.00       1.00          1.00\nsupport   349.00 188.00      1.00     537.00        537.00\nTESTING RESULTS: \n===============================\nCONFUSION MATRIX:\n[[123  28]\n [ 29  51]]\nACCURACY SCORE:\n0.7532\nCLASSIFICATION REPORT:\n               0     1  accuracy  macro avg  weighted avg\nprecision   0.81  0.65      0.75       0.73          0.75\nrecall      0.81  0.64      0.75       0.73          0.75\nf1-score    0.81  0.64      0.75       0.73          0.75\nsupport   151.00 80.00      0.75     231.00        231.00\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"scores['Random Forest'] = {\n        'Train': accuracy_score(y_train, rf_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, rf_clf.predict(X_test)),\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:20.439756Z","iopub.execute_input":"2025-04-04T09:17:20.440019Z","iopub.status.idle":"2025-04-04T09:17:20.633284Z","shell.execute_reply.started":"2025-04-04T09:17:20.439997Z","shell.execute_reply":"2025-04-04T09:17:20.631989Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## 3. Extra Trees\nExtra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n\nYou can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n\n**ExtraTreeClassifier**:\n\nThis class implements a meta-estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n**ExtraTreeClassifier Parameters**:\n- `n_estimators`: The number of trees in the forest.\n*** \n- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n***\n- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then an unlimited number of leaf nodes.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise, it is a leaf.\n***\n- `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n***\n- `oob_score`: Whether to use out-of-bag samples to estimate the generalization accuracy.\n***\n- `warm_start`: When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nex_tree_clf = ExtraTreesClassifier(n_estimators=1000, max_features=7, random_state=42)\nex_tree_clf.fit(X_train, y_train)\nevaluate(ex_tree_clf, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:32.841923Z","iopub.execute_input":"2025-04-04T09:17:32.842300Z","iopub.status.idle":"2025-04-04T09:17:34.760242Z","shell.execute_reply.started":"2025-04-04T09:17:32.842271Z","shell.execute_reply":"2025-04-04T09:17:34.759118Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TRAINIG RESULTS: \n===============================\nCONFUSION MATRIX:\n[[349   0]\n [  0 188]]\nACCURACY SCORE:\n1.0000\nCLASSIFICATION REPORT:\n               0      1  accuracy  macro avg  weighted avg\nprecision   1.00   1.00      1.00       1.00          1.00\nrecall      1.00   1.00      1.00       1.00          1.00\nf1-score    1.00   1.00      1.00       1.00          1.00\nsupport   349.00 188.00      1.00     537.00        537.00\nTESTING RESULTS: \n===============================\nCONFUSION MATRIX:\n[[124  27]\n [ 25  55]]\nACCURACY SCORE:\n0.7749\nCLASSIFICATION REPORT:\n               0     1  accuracy  macro avg  weighted avg\nprecision   0.83  0.67      0.77       0.75          0.78\nrecall      0.82  0.69      0.77       0.75          0.77\nf1-score    0.83  0.68      0.77       0.75          0.78\nsupport   151.00 80.00      0.77     231.00        231.00\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"scores['Extra Tree'] = {\n        'Train': accuracy_score(y_train, ex_tree_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, ex_tree_clf.predict(X_test)),\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:34.761624Z","iopub.execute_input":"2025-04-04T09:17:34.761896Z","iopub.status.idle":"2025-04-04T09:17:34.973413Z","shell.execute_reply.started":"2025-04-04T09:17:34.761873Z","shell.execute_reply":"2025-04-04T09:17:34.972217Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Boosting Algorithms\nBoosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n\nOnce created, the models make predictions that may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n\nThe two most common boosting ensemble machine learning algorithms are:\n\n1. AdaBoost\n2. Stochastic Gradient Boosting\n***","metadata":{}},{"cell_type":"markdown","source":"## 1. AdaBoost\nAdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay less attention to them in the construction of subsequent models.\n\nYou can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n\n**AdaBoostClassifier**:\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n\n**AdaBoostClassifier Params**:\n- `base_estimator`: The base estimator from which the boosted ensemble is built.\n***\n- `n_estimators`: The maximum number of estimators at which boosting is terminated. In case of the perfect fit, the learning procedure is stopped early.\n***\n- `learning_rate`: The learning rate shrinks the contribution of each classifier by ``learning_rate``. There is a trade-off between ``learning_rate`` and ``n_estimators``.\n***\n- `algorithm`: If 'SAMME.R' then use the SAMME.R real boosting algorithm. ``base_estimator`` must support the calculation of class probabilities. If 'SAMME' then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nada_boost_clf = AdaBoostClassifier(n_estimators=30)\nada_boost_clf.fit(X_train, y_train)\nevaluate(ada_boost_clf, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:47.306877Z","iopub.execute_input":"2025-04-04T09:17:47.307390Z","iopub.status.idle":"2025-04-04T09:17:47.485816Z","shell.execute_reply.started":"2025-04-04T09:17:47.307345Z","shell.execute_reply":"2025-04-04T09:17:47.484632Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TRAINIG RESULTS: \n===============================\nCONFUSION MATRIX:\n[[310  39]\n [ 51 137]]\nACCURACY SCORE:\n0.8324\nCLASSIFICATION REPORT:\n               0      1  accuracy  macro avg  weighted avg\nprecision   0.86   0.78      0.83       0.82          0.83\nrecall      0.89   0.73      0.83       0.81          0.83\nf1-score    0.87   0.75      0.83       0.81          0.83\nsupport   349.00 188.00      0.83     537.00        537.00\nTESTING RESULTS: \n===============================\nCONFUSION MATRIX:\n[[123  28]\n [ 27  53]]\nACCURACY SCORE:\n0.7619\nCLASSIFICATION REPORT:\n               0     1  accuracy  macro avg  weighted avg\nprecision   0.82  0.65      0.76       0.74          0.76\nrecall      0.81  0.66      0.76       0.74          0.76\nf1-score    0.82  0.66      0.76       0.74          0.76\nsupport   151.00 80.00      0.76     231.00        231.00\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"scores['AdaBoost'] = {\n        'Train': accuracy_score(y_train, ada_boost_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, ada_boost_clf.predict(X_test)),\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:48.329589Z","iopub.execute_input":"2025-04-04T09:17:48.329961Z","iopub.status.idle":"2025-04-04T09:17:48.363039Z","shell.execute_reply.started":"2025-04-04T09:17:48.329927Z","shell.execute_reply":"2025-04-04T09:17:48.361520Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## 2. Stochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) is one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the best techniques available for improving performance via ensembles.\n\n**GradientBoostingClassifier**:\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\n**GradientBoostingClassifier Parameters**:\n\n- `loss`: loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm.\n***\n- `learning_rate`: learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators.\n***\n- `n_estimators`: The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n***\n- `subsample`: The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias.\n***\n- `criterion`: The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with an improvement score by Friedman, \"`mse`\" for the mean squared error, and \"`mae`\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_depth`: maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise, it is a leaf.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then an unlimited number of leaf nodes.\n***\n- `warm_start`: When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution.\n***\n- `validation_fraction`: The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if ``n_iter_no_change`` is set to an integer.\n***\n- `n_iter_no_change`: used to decide if early stopping will be used to terminate training when the validation score is not improving. By default, it is set to None to disable early stopping. If set to a number, it will set aside the ``validation_fraction`` size of the training data as validation and terminate training when the validation score is not improving in all of the previous ``n_iter_no_change`` numbers of iterations. The split is stratified.\n***\n- `tol`: Tolerance for the early stopping. When the loss is not improving by at least tol for ``n_iter_no_change`` iterations (if set to a number), the training stops.\n***\n- `ccp_alpha`: Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen.\n***","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngrad_boost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngrad_boost_clf.fit(X_train, y_train)\nevaluate(grad_boost_clf, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:50.665551Z","iopub.execute_input":"2025-04-04T09:17:50.665992Z","iopub.status.idle":"2025-04-04T09:17:50.873657Z","shell.execute_reply.started":"2025-04-04T09:17:50.665949Z","shell.execute_reply":"2025-04-04T09:17:50.872438Z"},"trusted":true},"outputs":[{"name":"stdout","text":"TRAINIG RESULTS: \n===============================\nCONFUSION MATRIX:\n[[342   7]\n [ 19 169]]\nACCURACY SCORE:\n0.9516\nCLASSIFICATION REPORT:\n               0      1  accuracy  macro avg  weighted avg\nprecision   0.95   0.96      0.95       0.95          0.95\nrecall      0.98   0.90      0.95       0.94          0.95\nf1-score    0.96   0.93      0.95       0.95          0.95\nsupport   349.00 188.00      0.95     537.00        537.00\nTESTING RESULTS: \n===============================\nCONFUSION MATRIX:\n[[116  35]\n [ 26  54]]\nACCURACY SCORE:\n0.7359\nCLASSIFICATION REPORT:\n               0     1  accuracy  macro avg  weighted avg\nprecision   0.82  0.61      0.74       0.71          0.74\nrecall      0.77  0.68      0.74       0.72          0.74\nf1-score    0.79  0.64      0.74       0.72          0.74\nsupport   151.00 80.00      0.74     231.00        231.00\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"scores['Gradient Boosting'] = {\n        'Train': accuracy_score(y_train, grad_boost_clf.predict(X_train)),\n        'Test': accuracy_score(y_test, grad_boost_clf.predict(X_test)),\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-04T09:17:51.560990Z","iopub.execute_input":"2025-04-04T09:17:51.561402Z","iopub.status.idle":"2025-04-04T09:17:51.576346Z","shell.execute_reply.started":"2025-04-04T09:17:51.561365Z","shell.execute_reply":"2025-04-04T09:17:51.575102Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}